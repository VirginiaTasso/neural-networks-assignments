# -*- coding: utf-8 -*-
"""HW7_Tasso_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A92bmX-4FDFhDd6ZqW_ICn-KgrDqHI7h
"""

# *****************************************************************************
# *****************************************************************************
# k-means using a neural network
# *****************************************************************************
# *****************************************************************************

# *****************************************************************************
# Preamble and dataset loading, based on PyTorch tutorial
# *****************************************************************************
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import random
import numpy as np
import matplotlib.pyplot as plt

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

torch.set_default_device(device)
print(f"Using {device} device")

training_data = datasets.MNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.MNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)

batch_size = 64

train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)

# *****************************************************************************
# Building the neural network
# *****************************************************************************
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.centers = nn.Parameter(torch.zeros((10,28*28)))
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        z = self.flatten(x)
        x = torch.matmul(z, self.centers.T) - 0.5 * torch.sum(self.centers**2, dim=1)
        x = 20*x
        x = self.softmax(x)
        x = torch.matmul(x, self.centers) # reconstruction using cluster centers
        error = x - z
        return error

model = NeuralNetwork().to(device)


# *****************************************************************************
# Train and test loops
# *****************************************************************************
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    # Set the model to training mode - important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X.to(device))
        loss = loss_fn(pred, torch.zeros_like(pred)) # Comparison between the model's error and a 0-tensor with the same shape

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * batch_size + len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test_loop(dataloader, model, loss_fn):
    # Set the model to evaluation mode - important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.eval()
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0
    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode
    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True
    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X.to(device))
            test_loss += loss_fn(pred, torch.zeros_like(pred)).item()
    test_loss /= num_batches
    print(f"Avg loss: {test_loss:>8f} \n {size} {num_batches}")


# *****************************************************************************
# Optimization prameters and initialization
# *****************************************************************************
basic_train_dataloader = DataLoader(training_data, batch_size=1)
training_size = len(basic_train_dataloader.dataset)

loss_fn = nn.MSELoss() # use MSE loss
learning_rate = 4.5
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# ******************************************************************************
# ********** Point 1b: initialize cluster centers to individual images ********* #
# ******************************************************************************

'''with torch.no_grad():
  i = 0
  for X, y in basic_train_dataloader:
    if y == i:
      model.centers[i] = X.flatten()
      i += 1
    if i==10:
      break'''


with torch.no_grad():
  model.centers[:] = torch.zeros((10,28*28)).to(device)
  i = 0
  for X, y in basic_train_dataloader:
    if y == i:
      model.centers[i,:] = X.view(-1).to(device)
      i += 1
    if i==10:
      break



# ********** Standard training epochs ********* #

print(model)
print("Training model...")
epochs = 10 # train the model for 10 epochs
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print("Done!")

# *****************************************************************************
# Building the confusion matrix
# *****************************************************************************
print("Computing the confusion matrix...")
C = model.centers.detach().cpu() # Extract the centers from the model
counts = np.zeros((10, 10))# Initialize counts matrix
with torch.no_grad():
  for X, y in basic_train_dataloader:
    best_distance = 1e16 # Very large number for initialization
    best_index = 0 # Arbitrary index for initialization
    for j in range(10):
      dist = torch.sum((X.flatten() - model.centers[j])**2) #!!! Fill in !!!# Calculate distance of X from center j
      if dist < best_distance: # Determine condition to update the distance and index
        best_distance = dist # Update the distance
        best_index = j # Update the index
    counts[y.item(), best_index] += 1 # Update the counts at the (label, cluster) index
print("Confusion Matrix: ")
print(counts.astype(int))

# *****************************************************************************
# Displaying the centers
# *****************************************************************************
print("Cluster centers:")
plt.figure(figsize=(10, 6))
for j in range(10):
  plt.subplot(2, 5, j+1)
  print(f"Cluster {j}")
  q = model.centers[j].detach().cpu().view(28, 28) # Grab center j
  plt.imshow(q, cmap = 'gray') # Display center j as an image
  plt.title(f"Cluster {j}", fontsize=15) # Set title for each subplot
  plt.axis('off') # Turn off axes for a cleaner look
plt.suptitle("Cluster Centers", fontsize = 20)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

import seaborn as sns
# *****************************************************************************
# Building the confusion matrix
# *****************************************************************************
counts = counts.astype(int)


print("Confusion Matrix: ")
print(counts)


plt.figure(figsize=(10, 8))
sns.heatmap(counts, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
plt.xlabel("Cluster Index")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# ********************************
# ********** Point 1c********* #
# ********************************
with torch.no_grad():
  model.centers[:] = torch.randn((10, 784)) # randomly initialize centroids within the range 0 and 1

# ********** Standard training epochs ********* #

print(model)
print("Training model...")
epochs = 10 # train the model for 10 epochs
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print("Done!")

print("Cluster centers with random initialization:")
plt.figure(figsize=(6, 12))  # Adjust figure size for better visibility
for j in range(10):
    plt.subplot(2, 5, j + 1)  # Create a grid of 5 rows and 2 columns
    q = model.centers[j].detach().cpu().view(28, 28)  # Grab center j
    plt.imshow(q, cmap='gray')  # Display center j as an image
    plt.title(f"Cluster {j}", fontsize=12)
    plt.axis('off')  # Turn off axes for a cleaner look

# Add a general title for the entire figure
plt.suptitle("Cluster Centers with Random Initialization", fontsize=16, y=0.92)  # General title
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the general title
plt.show()

print("Cluster centers with random initialization:")
plt.figure(figsize=(10, 6))  # Adjust figure size for better visibility
for j in range(10):
    plt.subplot(2, 5, j + 1)  # Create a grid of 5 rows and 2 columns
    q = model.centers[j].detach().cpu().view(28, 28)  # Grab center j
    plt.imshow(q, cmap='gray')  # Display center j as an image
    plt.title(f"Cluster {j}", fontsize=12)
    plt.axis('off')  # Turn off axes for a cleaner look

# Add a general title for the entire figure
plt.suptitle("Cluster Centers with Random Initialization", fontsize=20, y=0.92)  # General title
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the general title
plt.show()